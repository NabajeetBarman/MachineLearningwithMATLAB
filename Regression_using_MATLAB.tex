% This LaTeX was auto-generated from MATLAB code.
% To make changes, update the MATLAB code and export to LaTeX again.

\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{color}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{epstopdf}
\usepackage[table]{xcolor}
\usepackage{matlab}

\sloppy
\epstopdfsetup{outdir=./}
\graphicspath{ {./Regression_using_MATLAB_images/} }

\matlabhastoc

\matlabmultipletitles

\begin{document}

\label{T_7A0D78DA}
\matlabtitle{\textbf{Regression Modeling using MATLAB}}

\begin{par}
\begin{flushleft}
This MATLAB Live Script is designed for the MA6600 Artificial Intelligence and Machine Learning Module for the UG Mathematics degree programm at Kingston University.
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
Copyright @ \textit{Nabajeet Barman (nabajeetbarman4@gmail.com)}
\end{flushleft}
\end{par}

\matlabtableofcontents{Table of Contents}

\begin{par}
\begin{flushleft}
Regression model is a machine learning model that outputs a continuous numeric response. Eg: Predicting stock prices is a regression problem.
\end{flushleft}
\end{par}


\vspace{1em}
\begin{par}
\begin{flushleft}
\includegraphics[width=\maxwidth{51.98193677872554em}]{image_0}
\end{flushleft}
\end{par}

\label{H_D16B6B58}
\begin{par}
\begin{flushleft}
                                                     \href{https://www.mathworks.com/help/stats/regression-learner-app.html}{Source}
\end{flushleft}
\end{par}

\label{H_D1280EDA}
\matlabheading{Types of Regression Models}

\begin{itemize}
\setlength{\itemsep}{-1ex}
   \item{\begin{flushleft} Linear Regression \end{flushleft}}
   \item{\begin{flushleft} Decision Trees \end{flushleft}}
   \item{\begin{flushleft} Support Vector Machines \end{flushleft}}
   \item{\begin{flushleft} Gaussian Process Regression \end{flushleft}}
\end{itemize}

\label{H_5C8301DA}
\matlabheading{Evaluating Regression Models}

\begin{itemize}
\setlength{\itemsep}{-1ex}
   \item{\begin{flushleft} Mean Absolute Error (MAE): The average magnitude of the residuals \end{flushleft}}
   \item{\begin{flushleft} Mean Square Error (MSE): The average of the squared residuals \end{flushleft}}
   \item{\begin{flushleft} Root MSE (RMSE): The square root of MSE \end{flushleft}}
   \item{\begin{flushleft} R-squared \end{flushleft}}
\end{itemize}

\begin{par}
\begin{flushleft}
MAE, MSE and RMSE are measures of error and are useful for comparing between models.
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
R-Squared is a measure of goodness of fit vs. a flat line model. The values range between 0 and 1.
\end{flushleft}
\end{par}

\label{H_86950227}
\matlabheading{Evaluating your model in MATLAB}

\begin{par}
\begin{flushleft}
Use metrics such as MAE, RMSE and R-squared to evaluate and compare the different regression models.
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
Use different visualization charts to get an understanding of how your model is performing.
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
Look for overall biases and areas where the model is performing poorly. One should consider the model's application focus when evaluating its performance.
\end{flushleft}
\end{par}


\label{T_0B1CEBCA}
\matlabtitle{The Boston House Price Dataset}

\begin{par}
\begin{flushleft}
The Boston House Price Dataset involves the prediction of a house price in thousands of dollars given details of the house and its neighborhood. It is a regression problem. There are 506 observations with 13 input variables and 1 output variable. The variable names are as follows:
\end{flushleft}
\end{par}


\vspace{1em}
\begin{enumerate}
\setlength{\itemsep}{-1ex}
   \item{\begin{flushleft} \textit{CRIM (Crime Rate):} per capita crime rate by town. \end{flushleft}}
   \item{\begin{flushleft} \textit{ZN (Size of Residential Land)}: proportion of residential land zoned for lots over 25,000 sq.ft. \end{flushleft}}
   \item{\begin{flushleft} \textit{INDUS (Industrial Factor)}: proportion of nonretail business acres per town. \end{flushleft}}
   \item{\begin{flushleft} \textit{CHAS: Charles River dummy variable} (= 1 if tract bounds river; 0 otherwise).   \%\% should be deleted as not useful!!!! \end{flushleft}}
   \item{\begin{flushleft} \textit{NOX (Nitrogen Oxide Concentration)}: nitric oxides concentration (parts per 10 million). \end{flushleft}}
   \item{\begin{flushleft} \textit{RM (Average number of Rooms)}: average number of rooms per dwelling. \end{flushleft}}
   \item{\begin{flushleft} \textit{AGE (House Age)}: Number of house built prior to 1940. \end{flushleft}}
   \item{\begin{flushleft} \textit{DIS (Employment Center)}: weighted distances to five Boston employment centers. \end{flushleft}}
   \item{\begin{flushleft} \textit{RAD (Convenience)}: index of accessibility to radial highways. \end{flushleft}}
   \item{\begin{flushleft} \textit{TAX (Property Tax)}: full-value property-tax rate per \$10,000. \end{flushleft}}
   \item{\begin{flushleft} \textit{PTRATIO}: pupil-teacher ratio by town. \end{flushleft}}
   \item{\begin{flushleft} \textit{B:} 1000(Bk â€“ 0.63)\textasciicircum{}2 where Bk is the proportion of blacks by town. \end{flushleft}}
   \item{\begin{flushleft} \textit{LSTAT (Status of Population)}: \% lower status of the population. \end{flushleft}}
   \item{\begin{flushleft} \textit{\textbf{MEDV}}\textbf{: Median value of owner-occupied homes in \$1000s. } \end{flushleft}}
\end{enumerate}

\begin{par}
\begin{flushleft}
\texttt{\textbf{Note:}} The baseline performance of predicting the mean value is an \texttt{RMSE} of approximately \texttt{9.21 thousand dollars}.
\end{flushleft}
\end{par}

\label{H_5C7357BF}
\matlabheading{Downloading the Dataset }

\begin{matlabcode}
% code from https://www.mathworks.com/products/demos/machine-learning/boosted-regression.html
filename = 'housing.txt';
urlwrite('http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data',filename);
inputNames = {'CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT'};
outputNames = {'MEDV'};
housingAttributes = [inputNames,outputNames];
\end{matlabcode}


\begin{par}
\begin{flushleft}
Importing into the workspace in a proper format (as was done with the titanic dataset)
\end{flushleft}
\end{par}

\begin{matlabcode}
format_spec = '%8f%7f%8f%3f%8f%8f%7f%8f%4f%7f%7f%7f%7f%f%[^\n\r]';
file_id = fopen(filename,'r');
data_array = textscan(file_id, format_spec, 'Delimiter', '', 'WhiteSpace', '',  'ReturnOnError', false);
fclose(file_id);
housing = table(data_array{1:end-1}, 'VariableNames', {'CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT', 'MEDV'});

% Delete the file and clear temporary variables
clearvars filename format_spec file_id data_array ans;
delete housing.txt
\end{matlabcode}


\begin{matlabcode}
% define input variables (predictors) and output (target) variables -- for
% easier usage
X = housing{:,inputNames};
Y = housing{:,outputNames};
\end{matlabcode}


\label{H_8185C337}
\matlabheading{Model 01: Linear Regression}

\label{H_7B185E33}
\begin{par}
\begin{flushleft}
A linear regression model is an equation where each feature (predictor) is multiplied by a coefficient and summed together to predict the response. An additive constant, or intercept term is also included. 
\end{flushleft}
\end{par}

\begin{matlabcode}
mdl = fitlm(housing,'ResponseVar','MEDV')
\end{matlabcode}
\begin{matlaboutput}
mdl = 
Linear regression model:
    MEDV ~ 1 + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT

Estimated Coefficients:
                    Estimate        SE         tStat        pValue  
                   __________    _________    ________    __________

    (Intercept)        36.459       5.1035      7.1441    3.2834e-12
    CRIM             -0.10801     0.032865     -3.2865     0.0010868
    ZN                0.04642     0.013727      3.3816    0.00077811
    INDUS            0.020559     0.061496     0.33431       0.73829
    CHAS               2.6867      0.86158      3.1184      0.001925
    NOX               -17.767       3.8197     -4.6513    4.2456e-06
    RM                 3.8099      0.41793      9.1161    1.9794e-18
    AGE            0.00069222      0.01321    0.052402       0.95823
    DIS               -1.4756      0.19945      -7.398    6.0135e-13
    RAD               0.30605     0.066346      4.6129    5.0705e-06
    TAX             -0.012335    0.0037605       -3.28     0.0011116
    PTRATIO          -0.95275      0.13083     -7.2825    1.3088e-12
    B               0.0093117     0.002686      3.4668    0.00057286
    LSTAT            -0.52476     0.050715     -10.347    7.7769e-23


Number of observations: 506, Error degrees of freedom: 492
Root Mean Squared Error: 4.75
R-squared: 0.741,  Adjusted R-Squared: 0.734
F-statistic vs. constant model: 108, p-value = 6.72e-135
\end{matlaboutput}

\label{H_046AB3C1}
\matlabheadingtwo{\textbf{Choose a Fitting Method}}

\begin{par}
\begin{flushleft}
There are three ways to fit a model to data:
\end{flushleft}
\end{par}

\begin{itemize}
\setlength{\itemsep}{-1ex}
   \item{\begin{flushleft} \href{https://www.mathworks.com/help/stats/linear-regression-model-workflow.html#btcpwno}{Least-Squares Fit} \end{flushleft}}
   \item{\begin{flushleft} \href{https://www.mathworks.com/help/stats/linear-regression-model-workflow.html#btcpwmd}{Robust Fit} \end{flushleft}}
   \item{\begin{flushleft} \href{https://www.mathworks.com/help/stats/linear-regression-model-workflow.html#btcpwm_}{Stepwise Fit} \end{flushleft}}
\end{itemize}

\begin{par}
\begin{flushleft}
\textbf{Least-Squares Fit}
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
Use \href{https://www.mathworks.com/help/stats/fitlm.html}{\texttt{fitlm}} to construct a least-squares fit of a model to the data. This method is best when you are reasonably certain of the modelâ€™s form, and mainly need to find its parameters. This method is also useful when you want to explore a few models. The method requires you to examine the data manually to discard outliers, though there are techniques to help (see \href{https://www.mathworks.com/help/stats/linear-regression-model-workflow.html#btb50q6}{Examine Quality and Adjust Fitted Model}).
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
\textbf{Robust Fit}
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
Use \href{https://www.mathworks.com/help/stats/fitlm.html}{\texttt{fitlm}} with the \texttt{RobustOpts} name-value pair to create a model that is little affected by outliers. Robust fitting saves you the trouble of manually discarding outliers. However, \href{https://www.mathworks.com/help/stats/linearmodel.step.html}{\texttt{step}} does not work with robust fitting. This means that when you use robust fitting, you cannot search stepwise for a good model.
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
\textbf{Stepwise Fit}
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
Use \href{https://www.mathworks.com/help/stats/stepwiselm.html}{\texttt{stepwiselm}} to find a model, and fit parameters to the model. \href{https://www.mathworks.com/help/stats/stepwiselm.html}{\texttt{stepwiselm}} starts from one model, such as a constant, and adds or subtracts terms one at a time, choosing an optimal term each time in a greedy fashion, until it cannot improve further. Use stepwise fitting to find a good model, which is one that has only relevant terms. The result depends on the starting model. Usually, starting with a constant model leads to a small model. Starting with more terms can lead to a more complex model, but one that has lower mean squared error. See \href{https://www.mathworks.com/help/stats/stepwise-regression.html#bs9ss9b}{Compare large and small stepwise models}.
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
You cannot use robust options along with stepwise fitting.
\end{flushleft}
\end{par}


\begin{matlabcode}
% alternatively, using RobustOpts option!!!
mdl = fitlm(X,Y,'linear', 'RobustOpts','on')
\end{matlabcode}
\begin{matlaboutput}
mdl = 
Linear regression model (robust fit):
    y ~ 1 + x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13

Estimated Coefficients:
                   Estimate        SE         tStat        pValue  
                   _________    _________    ________    __________

    (Intercept)       7.1782        4.099      1.7512      0.080537
    x1              -0.12595     0.026397     -4.7715     2.414e-06
    x2              0.027216     0.011026      2.4684       0.01391
    x3             -0.014011     0.049392    -0.28366       0.77679
    x4                1.2367      0.69201      1.7872      0.074525
    x5               -6.2581        3.068     -2.0398      0.041902
    x6                6.2406      0.33567      18.591    7.9119e-59
    x7             -0.041768      0.01061     -3.9367    9.4516e-05
    x8              -0.96665       0.1602     -6.0341    3.1478e-09
    x9               0.15467     0.053288      2.9025     0.0038679
    x10            -0.011302    0.0030204     -3.7419    0.00020414
    x11             -0.70621      0.10508     -6.7208    5.0149e-11
    x12             0.012506    0.0021573      5.7972    1.2065e-08
    x13             -0.21653     0.040734     -5.3158    1.6137e-07


Number of observations: 506, Error degrees of freedom: 492
Root Mean Squared Error: 3.81
R-squared: 0.821,  Adjusted R-Squared: 0.816
F-statistic vs. constant model: 174, p-value = 2.36e-174
\end{matlaboutput}
\begin{matlabcode}
% 'y ~ x1 + x2 + x3' is a three-variable linear model with intercept.
% 'y ~ x1 + x2 + x3 - 1' is a three-variable linear model without intercept.
\end{matlabcode}


\label{H_833F55DC}
\matlabheadingtwo{\textbf{Examine Quality and Adjust Fitted Model}}

\label{H_F3D56B63}
\begin{matlabcode}
tbl = anova(mdl)
\end{matlabcode}
\begin{matlabtableoutput}
{
\begin{tabular} {|c|c|c|c|c|c|}\hline
\mlcell{ } & \mlcell{SumSq} & \mlcell{DF} & \mlcell{MeanSq} & \mlcell{F} & \mlcell{pValue} \\ \hline
\mlcell{1 x1} & \mlcell{330.7264} & \mlcell{1} & \mlcell{330.7264} & \mlcell{22.7673} & \mlcell{0.0000} \\ \hline
\mlcell{2 x2} & \mlcell{88.5107} & \mlcell{1} & \mlcell{88.5107} & \mlcell{6.0931} & \mlcell{0.0139} \\ \hline
\mlcell{3 x3} & \mlcell{1.1688} & \mlcell{1} & \mlcell{1.1688} & \mlcell{0.0805} & \mlcell{0.7768} \\ \hline
\mlcell{4 x4} & \mlcell{46.3970} & \mlcell{1} & \mlcell{46.3970} & \mlcell{3.1940} & \mlcell{0.0745} \\ \hline
\mlcell{5 x5} & \mlcell{60.4424} & \mlcell{1} & \mlcell{60.4424} & \mlcell{4.1609} & \mlcell{0.0419} \\ \hline
\mlcell{6 x6} & \mlcell{5.0209e+03} & \mlcell{1} & \mlcell{5.0209e+03} & \mlcell{345.6378} & \mlcell{0.0000} \\ \hline
\mlcell{7 x7} & \mlcell{225.1265} & \mlcell{1} & \mlcell{225.1265} & \mlcell{15.4977} & \mlcell{0.0001} \\ \hline
\mlcell{8 x8} & \mlcell{528.9053} & \mlcell{1} & \mlcell{528.9053} & \mlcell{36.4099} & \mlcell{0.0000} \\ \hline
\mlcell{9 x9} & \mlcell{122.3789} & \mlcell{1} & \mlcell{122.3789} & \mlcell{8.4246} & \mlcell{0.0039} \\ \hline
\mlcell{10 x10} & \mlcell{203.4007} & \mlcell{1} & \mlcell{203.4007} & \mlcell{14.0021} & \mlcell{0.0002} \\ \hline
\mlcell{11 x11} & \mlcell{656.1529} & \mlcell{1} & \mlcell{656.1529} & \mlcell{45.1697} & \mlcell{0.0000} \\ \hline
\mlcell{12 x12} & \mlcell{488.1935} & \mlcell{1} & \mlcell{488.1935} & \mlcell{33.6073} & \mlcell{0.0000} \\ \hline
\mlcell{13 x13} & \mlcell{410.4803} & \mlcell{1} & \mlcell{410.4803} & \mlcell{28.2575} & \mlcell{0.0000} \\ \hline
\mlcell{14 Error} & \mlcell{7.1470e+03} & \mlcell{492} & \mlcell{14.5264} & \mlcell{1.0000} & \mlcell{0.5000} \\ 
\hline
\end{tabular}
}
\end{matlabtableoutput}


\begin{matlabcode}
% Diagnostic plots help you identify outliers, and see other problems in your model or fit. 
plotDiagnostics(mdl)
\end{matlabcode}
\begin{center}
\includegraphics[width=\maxwidth{56.196688409433015em}]{figure_0.png}
\end{center}

\begin{par}
\begin{flushleft}
An effect \textbf{leverage plot}, also known as added variable \textbf{plot} or partial regression \textbf{leverage plot}, shows the unique effect of a term in the model. A horizontal line shows the constrained model without the term; a slanted line shows the unconstrained model with the term. The points with high extreme values of X are said to have high leverage. High leverage points have a greater ability to move the fitted line. If these points fall outside the overall pattern, they can be influential. 
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
There are a few points with high leverage. But this plot does not reveal whether the high-leverage points are outliers.
\end{flushleft}
\end{par}

\label{H_DA32B37E}
\begin{par}
\begin{flushleft}
\textbf{What to do in case if a point is very influential?}
\end{flushleft}
\end{par}

\label{H_76F03B8E}
\begin{par}
\begin{flushleft}
First, make sure that the observation is correctly recorded - that the data is as intended. Also check, if the conclusions change in case of exclusion/inclusion of the high leverage point(s). Can we get more observations near that value of X?
\end{flushleft}
\end{par}


\label{H_9D9CDDF6}
\matlabheadingthree{\textbf{Plot with Cook's distance}}

\begin{par}
\begin{flushleft}
In statistics, \textbf{Cook's distance} (often referred to as \textbf{Cook's D}) is a common measurement of a data point's influence. It's a way to find influential outliers in a set of predictor variables when performing a least-squares regression analysis. The concept was introduced by an American statistician named R.
\end{flushleft}
\end{par}

\begin{matlabcode}
plotDiagnostics(mdl,'cookd')
\end{matlabcode}
\begin{center}
\includegraphics[width=\maxwidth{56.196688409433015em}]{figure_1.png}
\end{center}


\label{H_4F781023}
\matlabheadingthree{\textbf{Residuals â€” Model Quality for Training Data}}

\begin{par}
\begin{flushleft}
There are several residual plots to help you discover errors, outliers, or correlations in the model or data. The simplest residual plots are the default histogram plot, which shows the range of the residuals and their frequencies, and the probability plot, which shows how the distribution of the residuals compares to a normal distribution with matched variance.
\end{flushleft}
\end{par}

\begin{matlabcode}
plotResiduals(mdl)
\end{matlabcode}
\begin{center}
\includegraphics[width=\maxwidth{56.196688409433015em}]{figure_2.png}
\end{center}

\label{H_CACF9D54}
\begin{par}
\begin{flushleft}
The observations above 28 and below -10 are potential outliers!!!!
\end{flushleft}
\end{par}


\begin{matlabcode}
plotResiduals(mdl,'probability')
\end{matlabcode}
\begin{center}
\includegraphics[width=\maxwidth{56.196688409433015em}]{figure_3.png}
\end{center}

\begin{par}
\begin{flushleft}
The potential outliers appear on this plot as well (bottom left most and top right ones). 
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
Otherwise, the probability plot seems reasonably straight, meaning a reasonable fit to normally distributed residuals.
\end{flushleft}
\end{par}


\label{H_A4D72892}
\matlabheading{Model 02: Regression Tree}

\label{H_676956F7}
\begin{par}
\begin{flushleft}
Regression trees pass observations through a sequence of binary conditions on the predictor values called â€˜nodesâ€™. A particular sequence of nodes is called a â€˜branchâ€™. The nodes at the tip of each branch are called â€˜leavesâ€™, where a fixed response value is predicted for all observations that reach this leaf. 
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
Create nodes by finding a binary condition on a predictor that best splits the training observations that reach the node into two groups. The â€˜bestâ€™ split is determined by calculating the sum of squared residuals for both groups. The group mean is used as the 'predictedâ€™ value. Nodes are considered leaves when they canâ€™t be split further. 
\end{flushleft}
\end{par}

\begin{par}
\begin{flushleft}
Trees are very flexible and do not require:
\end{flushleft}
\end{par}

\begin{itemize}
\setlength{\itemsep}{-1ex}
   \item{\begin{flushleft} Polynomial features to capture nonlinear predictor-response relationships \end{flushleft}}
   \item{\begin{flushleft} Additional features for categorical predictors \end{flushleft}}
   \item{\begin{flushleft} Standardization of predictor values \end{flushleft}}
\end{itemize}

\begin{par}
\begin{flushleft}
Trees may not capture interactions between predictors, though additional options can be specified to capture these terms.
\end{flushleft}
\end{par}

\begin{matlabcode}
% Using Cross Validation
rng(10); % For reproducibility - changing this will change the result. 
% SAME is true for the app - each time you redo the steps, it will produce results with different RMSE values
% Set aside 90% of the data for training
model = fitrtree(X,Y); % model is a RegressionTree model. % fitctree for classification trees
% Cross-validate the regression tree using 10-fold cross-validation.
cvmodel = crossval(model);
%cvmodel = crossval(model,'KFold',5); % default is 10
L = kfoldLoss(cvmodel); % 10-fold cross-validated mean-squared error.
fprintf('Root Mean-square testing error = %f\n',sqrt(L));
\end{matlabcode}
\begin{matlaboutput}
Root Mean-square testing error = 4.159023
\end{matlaboutput}


\begin{matlabcode}
% Using percent holdout
rng(10); % For reproducibility
% Set aside 90% of the data for training
cv = cvpartition(height(housing),'holdout',0.1);
mdl = fitrtree(X(cv.training,:),Y(cv.training,:),'MinLeafSize', 4, 'Surrogate', 'off');
L = loss(mdl,X(cv.test,:),Y(cv.test));
fprintf('Root Mean-square testing error = %f\n',sqrt(L));
\end{matlabcode}
\begin{matlaboutput}
Root Mean-square testing error = 3.458227
\end{matlaboutput}


\label{H_CFC10356}
\matlabheadingtwo{Plot Fit Against Test Data}

\label{H_6233F13D}
\begin{matlabcode}
%figure(1);
plot(Y(cv.test),'b','LineWidth',2), 
hold on
plot(predict(mdl,X(cv.test,:)),'r.-','LineWidth',1,'MarkerSize',15)
% Observe first hundred points, pan to view more
%xlim([0 100])
legend({'Actual','Predicted'})
xlabel('Training Data point');
ylabel('Median house price');
hold off
\end{matlabcode}
\begin{center}
\includegraphics[width=\maxwidth{56.196688409433015em}]{figure_4.png}
\end{center}
\begin{matlabcode}
% similarly you can do for the training dataset
\end{matlabcode}


\label{H_8FCB3809}
\matlabheadingtwo{Plotting the predictor importance}

\begin{matlabcode}
% Plot the predictors sorted on importance
[predictorImportance,sortedIndex] = sort(mdl.predictorImportance);
figure(2);
barh(predictorImportance)
set(gca,'ytickLabel',inputNames(sortedIndex))
xlabel('Predictor Importance')
\end{matlabcode}
\begin{center}
\includegraphics[width=\maxwidth{56.196688409433015em}]{figure_5.png}
\end{center}

\begin{par}
\begin{flushleft}
A good idea would be remove the very low important features and retrain your model (reducing both training and testing complexity).
\end{flushleft}
\end{par}


\label{H_1863EA4A}
\matlabheading{Model 03: Neural Network}

\label{H_EBCD3DDC}
\begin{par}
\begin{flushleft}
Data Preparation (generate the training set and data set)
\end{flushleft}
\end{par}

\begin{matlabcode}
% Cross varidation (train: 70%, test: 30%)
cv = cvpartition(size(housing,1),'HoldOut',0.3);
idx = cv.test;
% Separate to training and test data
dataTrain = housing(~idx,:);
dataTest  = housing(idx,:);
% training predictors and target 
pred_train = dataTrain{:,inputNames}';
target_train = dataTrain{:,outputNames}';
% test predictors and target 
pred_test = dataTest{:,inputNames}';
target_test = dataTest{:,outputNames}';

%%Neural Networks
% Create and train feedforward neural network
net = feedforwardnet(10, 'trainlm');% create a network
%net.trainParam.epochs = 1000;% set training times
%net.trainParam.goal = 0.0000001;% setting convergence error
[net, tr] = train (net, pred_train, target_train);% training network
% network simulation, test data
predict_test = sim(net, pred_test);% put into the network to output data
\end{matlabcode}

\begin{par}
\begin{flushleft}
Assessing the performance of the trained network
\end{flushleft}
\end{par}

\begin{matlabcode}
% The default performance function is mean squared error.
perf = perform(net,pred_test,target_test)
\end{matlabcode}
\begin{matlaboutput}
perf = 2.2961e+04
\end{matlaboutput}

\begin{par}
\begin{flushleft}
Result analysis
\end{flushleft}
\end{par}

\begin{matlabcode}
target_test = target_test';
err_prices = target_test-predict_test;% error
[mean(err_prices) std(err_prices)];% average, standard deviation
figure(1);
plot(target_test);
hold on;
plot(predict_test,'r.-','LineWidth',1,'MarkerSize',15);
%xlim([1 length(t_test)]);
hold off;
legend({'Actual','Predicted'})
xlabel('Test Data point');
ylabel('Median house price');
\end{matlabcode}
\begin{center}
\includegraphics[width=\maxwidth{56.196688409433015em}]{figure_6.png}
\end{center}

\end{document}
